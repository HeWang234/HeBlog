---
categories: [RESEARCH]
---

# 机器学习中的一些问题

## 基础概念

**CNN**

三个特点：

1. 局部感受野（Local Receptive Fields）
2. 共享权值(Shared Weights）
3. 池化（Pooling)

## 问题

三层的神经网络就可以拟合任何一个函数

mse过于精确？

分类的最后一层使用softmax，归一化

**激活函数作用？**

得到的网络可以模拟数据更复杂的关系和模式，拥有更好的泛化能力。

在实际进行激活函数的选择时：

- 首先尝试 ReLU，计算简单，速度快。
- 如果 ReLU 效果欠佳，尝试 Leaky ReLU 或 Maxout 等变种。
- 尝试 tanh 正切函数（以零点为中心，零点处梯度为1）。
- sigmoid / tanh 在 RNN（LSTM、注意力机制等）结构中有所应用，作为门控或者输出概率值。
- 在浅层神经网络中，如不超过4层的，可选择使用多种激励函数，没有太大的影响。

**正则化的目的？**

正则化（regularization）是所有用来降低算法泛化误差（generalization error）的方法的总称。以提升 bias 为代价降低 variance。现实中效果最好的深度学习模型，往往是【复杂的模型（大且深）】+【有效的正则化】。

在算法中使用，是防止模型出现过拟合。

https://blog.csdn.net/Solo95/article/details/84586794

正则化方法（Regularizer）

1. Norm penalty：常用 L1/L2 regularization，理论机制不同，按特征方向重要性，L1 以阈值“削砍”参数分量，L2 “缩水”参数分量；深度学习中的实现方式有【weight decay】和【硬约束（重投影）】两种，各有不足。

2. L2-regularization 的特殊作用：解决【欠定问题】，调整协方差矩阵（covariance matrix）使其可逆。

3. Dropout layer 与 batchnorm layer：dropout 本质是一种 ensemble method；dropout 与 batchnorm 用于 regression 时有弊端；同时使用时理论上有冲突。

4. 深度学习的 Early stopping：减少 overfit 之后无意义的训练。 



归一化
提升模型精度：归一化后，不同维度之间的特征在数值上有一定比较性，可以大大提高分类器的准确性。

标准化

加速模型收敛：标准化后，最优解的寻优过程明显会变得平缓，更容易正确的收敛到最优解。

**增加噪声的目的？**

防止过拟和，提高泛化能力。https://www.zhihu.com/question/275255350/answer/1202608670
