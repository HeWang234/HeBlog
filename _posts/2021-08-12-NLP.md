---
title: NLP
categories: [Research]
---

# NLP

## 1 基础知识

[通俗理解word2vec](<https://www.jianshu.com/p/471d9bfbd72f> )

[cs224n学习笔记(2)CBOW与Skip-Gram模型](<https://zhuanlan.zhihu.com/p/47585825> )

[从Word Embedding到Bert模型](https://zhuanlan.zhihu.com/p/49271699)

[CS224n-notes-and-codes](https://github.com/beyondguo/CS224n-notes-and-codes)

## 2 BERT

[源代码](https://github.com/google-research/bert)

[从零解读碾压循环神经网络的transformer模型(一)-注意力机制](https://www.bilibili.com/video/av58239477/?spm_id_from=333.788.b_636f6d6d656e74.4) 很好的讲解视频，[文档](https://github.com/aespresso/a_journey_into_math_of_ml)也较全面

[一文读懂BERT(原理篇)](https://blog.csdn.net/jiaowoshouzi/article/details/89073944) 很棒的一篇博客

The Illustrated Transformer https://jalammar.github.io/illustrated-transformer/ 较为优秀的blog，国内大多数博客抄的这篇，但感觉流程逻辑较分散

[词向量之BERT](<https://zhuanlan.zhihu.com/p/48612853> )

[Attention Is All you Need](<https://senliuy.gitbook.io/advanced-deep-learning/di-er-zhang-ff1a-xu-lie-mo-xing/attention-is-all-you-need> )

[Bert在NLP各领域的应用进展](https://zhuanlan.zhihu.com/p/68446772) 一个很好的展望，新颖深入理性幽默，是我喜欢的博主风格hh

输入：[CLS] 映射为NSP二分类任务、[SEP] 分句

预训练任务：MLM+NSP



## 词向量

### word2vec

- skip-gram 
- CBOW 在窗口内预测中间词

### FastText

类似于CBOW，预测标签

softmax 归一化处理 较耗时    --> 分层的softmax 根据类别的频率构造霍夫曼树

n-gram 字粒度和词粒度
